import os
import numpy as np
import sys
import pandas as pd
from scipy.spatial.distance import cosine
#import json
from sentence_transformers import SentenceTransformer
import nltk
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from typing import List, Dict, Any

def extract_key_sentences_tfidf(text: str, top_k: int = 10) -> List[str]:
    """TF-IDFë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ë¬¸ì¥ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    sentences = sent_tokenize(text)

    if len(sentences) <= top_k:
        return sentences
    
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences)
    scores = tfidf_matrix.sum(axis=1)

    scored_sentences = [(sentences[i], scores[i, 0]) for i in range(len(sentences))]
    top_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:top_k]
    return [s for s, _ in top_sentences]

def get_summary_embedding(text_content: str, model: SentenceTransformer, top_k: int = 10) -> np.ndarray:
    """ì£¼ìš” ë¬¸ì¥ë“¤ì„ ì¶”ì¶œí•œ ë’¤ í‰ê·  ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if not text_content.strip():
        embedding_dim = model.get_sentence_embedding_dimension()
        return np.zeros(embedding_dim, dtype=np.float32)
    key_sentences = extract_key_sentences_tfidf(text_content, top_k=top_k)
    embeddings = model.encode(key_sentences, convert_to_numpy=True)
    return np.mean(embeddings, axis=0)

def transform_syllabus_to_classifier_format(syllabus_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    transformed_data = []
    for syllabus in syllabus_list:
        # labelì„ class_name ë˜ëŠ” class_code ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •
        label = syllabus.get("syllabus_class_name") or "unknown_course"
        content_parts = [
            syllabus.get("syllabus_class_name", ""),
            syllabus.get("syllabus_class_code", ""),
            syllabus.get("syllabus_professor_name", ""),
            syllabus.get("syllabus_objectives", ""),
            syllabus.get("syllabus_description", ""),
            syllabus.get("syllabus_schedule", "")
        ]
        content = " ".join([part for part in content_parts if part])
        if content:
            transformed_data.append({
                "label": label,    # ì—¬ê¸°ì„œ labelì´ ê°•ì˜ ì´ë¦„ìœ¼ë¡œ ë“¤ì–´ê°€ì•¼ í•¨
                "content": content
            })
    return transformed_data


# --- 2. ML ê¸°ë°˜ ë¶„ë¥˜ê¸° í´ë˜ìŠ¤ (í•µì‹¬ ë¡œì§) ---
def _load_sentence_bert_model(model_name: str) -> SentenceTransformer:
    """Sentence-BERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"\n{'='*50}\n ğŸ§  Sentence-BERT ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}\n{'='*50}")
    try:
        model = SentenceTransformer(model_name)
        print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        return model
    except Exception as e:
        print(f"[ì˜¤ë¥˜] Sentence-BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
        sys.exit(1)


class MLClassifier:
    """Sentence-BERT ëª¨ë¸ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""

    def __init__(self, syllabus: List[Dict[str, str]], model_name: str = 'sentence-transformers/all-mpnet-base-v2'):
        self.model = _load_sentence_bert_model(model_name)
        self.reference_data = transform_syllabus_to_classifier_format(syllabus)
        self.reference_data = self._embed_reference_data(self.reference_data)
        if not self.reference_data:
            print("[ê²½ê³ ] ì°¸ì¡° ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ì„ë² ë”©í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", file=sys.stderr)

    def _embed_reference_data(self, reference_data: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """ì°¸ì¡° ë°ì´í„°ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“š {len(reference_data)}ê°œì˜ ì°¸ì¡° íŒŒì¼ ë²¡í„°í™” ì‹œì‘...")
        embedded_data = []
        for item in reference_data:
            text = item.get("content", "")
            label = item.get("label", "unknown")
            if text:
                embedding = get_summary_embedding(text, self.model)
                embedded_data.append({
                    "label": label,
                    "embedding": embedding
                })
        print(f"âœ… {len(embedded_data)}ê°œì˜ ì°¸ì¡° íŒŒì¼ ë²¡í„°í™” ì™„ë£Œ.")
        return embedded_data

    def classify_single(self, file_data: str) -> Dict[str, str]:
        
        if not self.reference_data:
            return {"label": "ë¶„ë¥˜ë¶ˆê°€ (ì°¸ì¡°ë°ì´í„° ì—†ìŒ)", "details": {}}
        if not file_data:
            return {"label": "ë¯¸ë¶„ë¥˜ (ì½˜í…ì¸  ì—†ìŒ)", "details": {}}
        query_vec = get_summary_embedding(file_data, self.model)
        if query_vec.size == 0:
            return {"label": "ë¯¸ë¶„ë¥˜ (ì„ë² ë”© ì‹¤íŒ¨)", "details": {}}
        similarities = []
        for ref in self.reference_data:
            if ref["embedding"].size == 0:
                continue
            similarity = 1 - cosine(query_vec, ref["embedding"])

            similarities.append({
                "label": ref["label"],
                "similarity": similarity
            })


        if not similarities:
            return {"label": "ë¯¸ë¶„ë¥˜ (ìœ ì‚¬ë„ ê³„ì‚° ë¶ˆê°€)", "details": {}}
        
        sorted_similarities = sorted(similarities, key=lambda x: x["similarity"], reverse=True)
        top_1_match = sorted_similarities[0]
        top_1_label = top_1_match["label"]
        top_1_similarity = top_1_match["similarity"]

        SIMILARITY_THRESHOLD = 0.30
        DIFFERENCE_THRESHOLD = 0.02

        assigned_label = "ë¯¸ë¶„ë¥˜"
        details = {item['label']: item['similarity'] for item in sorted_similarities}

        if top_1_similarity >= SIMILARITY_THRESHOLD:
            if len(sorted_similarities) > 1:
                top_2_similarity = sorted_similarities[1]["similarity"]
                if (top_1_similarity - top_2_similarity) >= DIFFERENCE_THRESHOLD:
                    assigned_label = top_1_label
                else:
                    assigned_label = "ë¯¸ë¶„ë¥˜ (ìœ ì‚¬ë„ ì°¨ì´ ë¯¸ë‹¬)"
            else:
                assigned_label = top_1_label
        else:
            assigned_label = "ë¯¸ë¶„ë¥˜ (ìœ ì‚¬ë„ ì„ê³„ì¹˜ ë¯¸ë‹¬)"
        return {"label": assigned_label, "details": details}

    def classify(self, file_data: Dict[str, str]) -> str:
        """
        ë‹¨ì¼ íŒŒì¼ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ ML ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ê³ 
        ë¶„ë¥˜ëœ ë ˆì´ë¸”(ë¬¸ìì—´)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not file_data:
            print("â— ë¶„ë¥˜í•  íŒŒì¼ ë°ì´í„°ê°€ ì—†ì–´ ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return "ë¯¸ë¶„ë¥˜ (ì½˜í…ì¸  ì—†ìŒ)"
        
        content = file_data.get("rule_based_content", "")
        
        classification_result = self.classify_single(content)
        print(f" ë¶„ë¥˜ì™„ë£Œ!! ê°•ì˜ì´ë¦„: {file_data.get('file_name')}, {classification_result['label']}")
        
        return classification_result['label']